# `agent/oci_models.py`

## Overview
- Provides a thin wrapper to create OCI GenAI language model instances (either **ChatOCIGenAI** or **ChatOCIOpenAI**) based on configuration flags.  
- Supports both the native OCI LangChain integration and an OpenAI‑compatible path that uses `oci-openai` authentication.  
- Handles model‑specific quirks (e.g., models that do **not** accept `temperature` or `max_tokens`).  
- Offers a small debugging helper (`debug_llm`) that prints selected LLM attributes when `DEBUG` is enabled.  
- Relies on configuration values from `agent.config` and a private compartment identifier from `agent.config_private`.  

## Public API
| Symbol | Type | Description |
|--------|------|-------------|
| `get_llm` | function | Factory that returns a ready‑to‑use LLM object (`ChatOCIGenAI` or `ChatOCIOpenAI`) according to the current configuration. |
| `debug_llm` | function *(internal)* | Prints diagnostic information about an LLM instance; used only when `DEBUG` is `True`. |

## Key Behaviors and Edge Cases
| Situation | Behaviour |
|-----------|-----------|
| **Model supports kwargs** (`temperature`, `max_tokens`) | Passes a `model_kwargs` dict to `ChatOCIGenAI`. |
| **Model listed in `MODELS_WITHOUT_KWARGS`** | Skips `model_kwargs` (set to `None`) because those models reject the parameters. |
| **`USE_LANGCHAIN_OPENAI` is `False`** | Instantiates the classic OCI LangChain class `ChatOCIGenAI`. |
| **`USE_LANGCHAIN_OPENAI` is `True`** | Instantiates `ChatOCIOpenAI` (OpenAI‑compatible wrapper) with explicit `temperature` and `max_tokens` arguments. |
| **`DEBUG` flag** | When `True` and the OpenAI path is used, `debug_llm` prints the LLM’s class and selected attributes (model name, endpoint URLs, API key presence, etc.). |
| **Missing or malformed configuration** | The function will raise the underlying library’s exception (e.g., missing `COMPARTMENT_ID` or invalid `SERVICE_ENDPOINT`). No explicit validation is performed. |

## Inputs / Outputs / Side Effects
| Parameter | Type | Default | Meaning |
|-----------|------|---------|---------|
| `model_id` | `str` | `LLM_MODEL_ID` (from `agent.config`) | Identifier of the OCI GenAI model to load (e.g., `"cohere.command-r"`). |
| `temperature` | `float` | `TEMPERATURE` | Sampling temperature for generation (ignored for models in `MODELS_WITHOUT_KWARGS`). |
| `max_tokens` | `int` | `MAX_TOKENS` | Upper bound on generated token count (ignored for models in `MODELS_WITHOUT_KWARGS`). |

**Return:** an instantiated LLM object (`ChatOCIGenAI` or `ChatOCIOpenAI`).  
**Side effects:** may log debug information via the module‑level `logger`; may raise network‑related exceptions when the underlying client contacts OCI services.

## Usage Examples
```python
# Example 1 – default configuration (OCI native path)
from agent.oci_models import get_llm

llm = get_llm()                     # uses defaults from agent.config
response = llm.invoke("Explain quantum tunneling.")
print(response)

# Example 2 – explicit model and OpenAI‑compatible path
from agent.config import USE_LANGCHAIN_OPENAI, DEBUG
from agent.oci_models import get_llm

# Force the OpenAI wrapper and enable debug prints
USE_LANGCHAIN_OPENAI = True
DEBUG = True

llm = get_llm(model_id="openai.gpt-5", temperature=0.7, max_tokens=512)
answer = llm.invoke("Summarize the plot of *Inception*.")
print(answer)
```

## Risks / TODOs
- **Potential secret exposure:** The module imports `COMPARTMENT_ID` from `agent.config_private`. If that file contains hard‑coded credentials, they could be inadvertently committed. Ensure secrets are stored securely (e.g., environment variables, secret manager).  
- **No validation of configuration values:** Missing or malformed settings will surface as runtime errors from the OCI SDK. Adding explicit checks could improve robustness.  
- **Limited error handling:** Network failures, authentication errors, or unsupported model IDs are not caught; callers must handle exceptions.  
- **Debug helper prints directly to stdout:** In production, consider routing debug output through the logger instead of `print`.  

---  
*Generated by an automated documentation assistant.*
