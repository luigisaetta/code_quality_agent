# `agent/config.py`

## Overview
- Provides a single source of truth for runtime configuration of the LangGraph RAG demo.  
- Defines boolean flags for debugging and streaming output.  
- Stores authentication mode, LLM model selection, and generation parameters (temperature, top‑p, max tokens).  
- Holds OCI (Oracle Cloud Infrastructure) region information and builds the inference service endpoint URL.  
- Includes a switch to enable the newer LangChain‑OpenAI integration.  

## Public API
The module exports only **constants** intended to be imported by other parts of the application:

| Constant | Description |
|----------|-------------|
| `DEBUG` | Global flag to enable verbose debugging output (`True`/`False`). |
| `STREAMING` | Enables streaming responses from the LLM when `True`. |
| `AUTH` | Authentication method identifier (e.g., `"API_KEY"`). |
| `LLM_MODEL_ID` | Identifier of the default Large Language Model to use. |
| `TEMPERATURE` | Sampling temperature for the LLM (float, `0.0` disables randomness). |
| `TOP_P` | Nucleus sampling probability (`1` = no top‑p filtering). |
| `MAX_TOKENS` | Maximum number of tokens to generate per request. |
| `USE_LANGCHAIN_OPENAI` | Toggle to use the LangChain OpenAI integration (`True`/`False`). |
| `REGION` | OCI region string (e.g., `"us-chicago-1"`). |
| `SERVICE_ENDPOINT` | Fully‑qualified inference endpoint URL, derived from `REGION`. |

> **Note:** No functions or classes are defined; the module is purely declarative.

## Key Behaviors and Edge Cases
- **Dynamic endpoint construction:** `SERVICE_ENDPOINT` is built at import time using an f‑string that incorporates the current value of `REGION`. Changing `REGION` after import will **not** automatically update `SERVICE_ENDPOINT`; a manual recompute or module reload is required.
- **Authentication placeholder:** `AUTH` is set to `"API_KEY"` as a placeholder. Real deployments must replace this with a secure method (e.g., environment‑sourced API key or OCI IAM token).  
- **Model selection:** The default `LLM_MODEL_ID` points to an OpenAI‑compatible model (`"openai.gpt-oss-120b"`). Commented alternatives show how to switch to other vendor models.  
- **Debug/Streaming flags:** Turning `DEBUG` or `STREAMING` on may affect performance or expose sensitive data in logs; they should be used cautiously in production.  
- **LangChain integration toggle:** When `USE_LANGCHAIN_OPENAI` is `False`, code paths that rely on LangChain's OpenAI wrapper must be avoided or replaced with alternative back‑ends.

## Inputs / Outputs and Side Effects
- **Inputs:** No runtime inputs; all values are hard‑coded constants.  
- **Outputs:** The module provides configuration values for import; the only side effect is the creation of `SERVICE_ENDPOINT` during module import.  
- **Side Effects:** Importing the module evaluates the f‑string for `SERVICE_ENDPOINT`. No I/O, network calls, or mutable state beyond the constants.

## Usage Examples
```python
# Example 1: Basic import and read configuration
import agent.config as cfg

print("LLM model:", cfg.LLM_MODEL_ID)
print("Endpoint:", cfg.SERVICE_ENDPOINT)

# Example 2: Enable debugging for a script
import agent.config as cfg
cfg.DEBUG = True   # turn on verbose logging for the rest of the process

# Example 3: Override region and recompute endpoint (requires manual recompute)
import importlib
import agent.config as cfg

cfg.REGION = "eu-frankfurt-1"
# Re‑evaluate the endpoint after changing REGION
cfg.SERVICE_ENDPOINT = f"https://inference.generativeai.{cfg.REGION}.oci.oraclecloud.com"
print(cfg.SERVICE_ENDPOINT)
```

## Risks / TODOs
- **Potential secret exposure:** `AUTH = "API_KEY"` is a placeholder; ensure that real API keys are never hard‑coded. Load them securely from environment variables or secret managers.  
- **Stale endpoint after region change:** Updating `REGION` does not automatically refresh `SERVICE_ENDPOINT`. Consider converting `SERVICE_ENDPOINT` into a property or function to always reflect the current region.  
- **Debug flag in production:** Leaving `DEBUG = True` may leak internal state or sensitive data; enforce a safe default for production deployments.  
- **Future deprecation:** The module header warns that the API may change; downstream code should guard against breaking changes (e.g., by using `getattr` with defaults).  

---  
*Generated by a senior Python engineer, focusing on header analysis and secret scanning.*
